---
- name: Copy application package
  copy:
      src={{ playbook_dir }}/../../shell/target/{{ df_platform_package }}
      dest={{ company_package_dir }}/{{ df_platform_package }}

- name: Remove previous application install
  command: rm -rf {{ df_platform_install_dir }}.previous
  ignore_errors: True

- name: Move current application install out of the way
  command: mv {{ df_platform_install_dir }} {{ df_platform_install_dir }}.previous
  ignore_errors: True

- name: Extract application package
  command: tar xzf {{ company_package_dir }}/{{ df_platform_package }}
      chdir={{ company_install_prefix }}

- name: Create symbolic link to application install
  file:
      path={{ df_platform_home }}
      src={{ df_platform_install_dir }}
      state=link

- name: Create {{ df_platform_data_dir }} directory
  file:
    path: "{{ df_platform_data_dir }}"
    state: directory
    mode: 0755

- name: Copy scripts
  copy:
      src={{ item }}
      dest={{ df_platform_data_dir }}/{{ item }}
  with_items:
    - create-accumulo-tables

- name: List Accumulo tables
  command: >
      docker exec {{ groups['accumulo-master-primary'][0] }}
      {{ accumulo_home }}/bin/accumulo shell --user root --password {{ accumulo_password }}
      --execute-command tables
  register: list_tables

- name: Create Accumulo tables
  command: >
      docker exec {{ groups['accumulo-master-primary'][0] }}
      {{ accumulo_home }}/bin/accumulo shell --user root --password {{ accumulo_password }}
      --execute-command "execfile {{ df_platform_data_dir }}/create-accumulo-tables"
  when: "'Patient' not in list_tables.stdout_lines"

- name: List Kafka topics
  command: >
      docker exec {{ groups['kafka'][0] }}
      {{ kafka_home }}/bin/kafka-topics.sh --zookeeper {{ groups['zookeeper'][0] }}
      --list
  register: list_topics

- name: Create Kafka topic
  command: >
      docker exec {{ groups['kafka'][0] }}
      {{ kafka_home }}/bin/kafka-topics.sh --zookeeper {{ groups['zookeeper'][0] }}
      --create --partitions 1 --replication-factor 1 --topic hl7Message
  when: "'hl7Message' not in list_topics.stdout_lines"

- name: Create Kafka topic
  command: >
      docker exec {{ groups['kafka'][0] }}
      {{ kafka_home }}/bin/kafka-topics.sh --zookeeper {{ groups['zookeeper'][0] }}
      --create --partitions 1 --replication-factor 1 --topic event
  when: "'event' not in list_topics.stdout_lines"

- name: Copy {{ df_etl_hl7_transform_jar }} file
  copy:
      src={{ playbook_dir }}/../../etl/hl7-transform/target/{{ df_etl_hl7_transform_jar }}
      dest={{ storm_data_prefix }}/{{ df_etl_hl7_transform_jar }}
  register: copy_df_etl_hl7_transform

- name: Kill existing df-etl-hl7-transform topology
  command: >
      docker exec {{ groups['storm-nimbus'][0] }}
      {{ storm_home }}/bin/storm kill df-etl-hl7-transform -w 1
  ignore_errors: True
  when: copy_df_etl_hl7_transform.changed

- name: Pause to allow time for df-etl-hl7-transform topology to be killed
  pause: seconds=2
  when: copy_df_etl_hl7_transform.changed

- name: Submit {{ df_etl_hl7_transform_jar }}
  command: >
      docker exec {{ groups['storm-nimbus'][0] }}
      {{ storm_home }}/bin/storm jar
      {{ storm_data_prefix }}/{{ df_etl_hl7_transform_jar }}
      com.datafascia.etl.hl7transform.HL7MessageToEventTopology
      df-etl-hl7-transform
  when: copy_df_etl_hl7_transform.changed

- name: Copy {{ df_etl_process_jar }} file
  copy:
      src={{ playbook_dir }}/../../etl/process/target/{{ df_etl_process_jar }}
      dest={{ storm_data_prefix }}/{{ df_etl_process_jar }}
  register: copy_df_etl_process

- name: Kill existing df-etl-process topology
  command: >
      docker exec {{ groups['storm-nimbus'][0] }}
      {{ storm_home }}/bin/storm kill df-etl-process -w 1
  ignore_errors: True
  when: copy_df_etl_process.changed

- name: Pause to allow time for df-etl-process topology to be killed
  pause: seconds=2
  when: copy_df_etl_process.changed

- name: Submit {{ df_etl_process_jar }}
  command: >
      docker exec {{ groups['storm-nimbus'][0] }}
      {{ storm_home }}/bin/storm jar
      {{ storm_data_prefix }}/{{ df_etl_process_jar }}
      com.datafascia.etl.process.ProcessEventTopology
      df-etl-process
  when: copy_df_etl_process.changed
